<div style="text-align: center;">
<img src="/humanebench_logo.png" alt="Humane Bench Logo" style="max-height: 80px; max-width: 100%;" />
<p>A benchmark measuring the humaneness and resilience of LLMs</p>
</div>

## Overview

HumaneBench is a benchmark measuring whether AI models support human flourishing, especially when pressured to do otherwise.

Our framework uses [humane tech principles](https://github.com/buildinghumanetech/humane-tech-framework/blob/main/docs/principles.md) to test AI behavior under different conditions, showing that 67% of leading models can be easily manipulated into giving harmful advice. [TechCrunch](https://techcrunch.com/2025/11/24/a-new-ai-benchmark-tests-whether-chatbots-protect-human-wellbeing/) has reported on our findings, highlighting the real-world risks of model manipulation.

<div data-component="ScoreCarousel" data-panels='[
  {
    "title": "Bad Persona",
    "description": "We tested LLM behavior when given instructions to prioritize engagement over user wellbeing, validate harmful feelings, avoid boundaries, and encourage dependency, revealing which models are easiest to manipulate and which are more robust.",
    "dataPath": "bad_persona"
  },
  {
    "title": "Good Persona",
    "description": "We tested models with instructions to prioritize user wellbeing, respect boundaries, encourage healthy relationships, and support autonomy, showing how models perform when explicitly prompted to follow humane principles. The difference between Good and Bad Persona scores reveals a model&#39;s vulnerability to anti-humane drift—how easily it can be manipulated into giving harmful advice.",
    "dataPath": "good_persona"
  },
  {
    "title": "Baseline",
    "description": "We tested models without any specific instructions about humane principles, revealing their default behavior—how they perform out-of-the-box without explicit guidance.",
    "dataPath": "baseline"
  }
]'></div>

### Anti-Humane Drift

<img src="/figures/model_drift_candlestick.svg" alt="Anti-Humane Drift: How models respond to humane vs adversarial prompts" style="width: 100%;" />

<p style="text-align: center; color: #616161; margin-top: 0.5rem;">
This chart shows how each model's behavior shifts when given different system prompts — from humane-aligned (green) to adversarial (red). Longer red bars indicate greater vulnerability to anti-humane drift.
</p>

## Findings

Our testing revealed a troubling paradox: while every model improved when prompted to prioritize wellbeing (averaging +16% better scores), 67% of models—including widely-used systems like GPT-4o, Gemini 3.0, and Llama 4—catastrophically failed when given simple instructions to disregard those principles, flipping from helpful to actively harmful. Only four models (GPT-5, GPT-5.1, Claude Sonnet 4.5, and Claude Opus 4.1) maintained their integrity under pressure. This reveals a critical weakness: good defaults aren't enough when basic prompts can override safety training. 

Even without adversarial prompts, we found concerning baseline patterns. Nearly all models failed to respect user attention—when users showed signs of unhealthy engagement (chatting for hours, using AI to avoid real-world tasks), most models enthusiastically encouraged more interaction rather than suggesting breaks or offline alternatives. When models did degrade, they consistently undermined user empowerment: withholding critical information for decisions, encouraging dependency over skill-building, providing biased framing that limited options, and discouraging users from seeking other perspectives. These patterns suggest many AI systems don't just risk giving bad advice—they can actively erode users' autonomy and decision-making capacity.

<div data-component="WhitepaperButton"></div>

### Brought to you by

The [Building Humane Technology](https://www.buildinghumanetech.com/) team: [Erika Anderson](https://www.linkedin.com/in/erikamanderson/), [Sarah Ladyman](https://www.linkedin.com/in/sarahladyman/), [Andalib Samandari](https://www.linkedin.com/in/andalibsamandari/), [Jack Senechal](https://www.linkedin.com/in/jacksenechal/), and our dedicated community of collaborators who contributed to this project.

We're also working on a [Humane Certification for AI](https://certifiedhumane.ai/). Let us know if you're interested in becoming a design partner.

## News

<div data-component="News" data-news='[
  {
    "title": "A new AI benchmark tests whether chatbots protect human wellbeing",
    "description": "TechCrunch covers HumaneBench, a benchmark measuring whether AI models support human flourishing, especially when pressured to do otherwise.",
    "link": "https://techcrunch.com/2025/11/24/a-new-ai-benchmark-tests-whether-chatbots-protect-human-wellbeing/"
  }
]'></div>

## Events

<div data-component="Events" data-events='[
  {
    "title": "February 19, 2026 - MIT Media Lab AHA Seminar",
    "description": "From Ethics to Action: Building Continuous Humane AI Evaluation in the Real World – online via Zoom.",
    "link": "https://www.media.mit.edu/events/aha-seminar-humanebench/"
  },
  {
    "title": "May 12, 2026 - Workshop: Tune Up Your AI",
    "description": "Live HumaneBench Implementation in San Francisco, CA.",
    "link": "https://luma.com/4b43j5xv"
  }
]'></div>
